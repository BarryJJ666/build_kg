# -*- coding: utf-8 -*-
"""
éªŒè¯å’Œæµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•ä¼˜åŒ–ç‰ˆçš„æ€§èƒ½å’Œå‡†ç¡®æ€§
"""

import json
import time
from datetime import datetime
from neo4j import GraphDatabase
from config import NEO4J_CONFIG
import os


class Validator:
    """éªŒè¯å™¨"""

    def __init__(self):
        self.driver = GraphDatabase.driver(
            NEO4J_CONFIG['uri'],
            auth=(NEO4J_CONFIG['user'], NEO4J_CONFIG['password'])
        )

    def close(self):
        self.driver.close()

    def test_connection(self):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
        print("\n=== æµ‹è¯•æ•°æ®åº“è¿æ¥ ===")
        try:
            with self.driver.session() as session:
                result = session.run("RETURN 1 AS num")
                record = result.single()
                if record['num'] == 1:
                    print("âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ")
                    return True
        except Exception as e:
            print(f"âœ— æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False

    def get_patent_stats(self):
        """è·å–ä¸“åˆ©ç»Ÿè®¡"""
        print("\n=== ä¸“åˆ©æ•°æ®ç»Ÿè®¡ ===")
        with self.driver.session() as session:
            # æ€»æ•°
            result = session.run("""
                MATCH (p:Patent)
                WHERE p.titleZh IS NOT NULL
                RETURN count(p) AS total
            """)
            total = result.single()['total']
            print(f"ä¸“åˆ©æ€»æ•°: {total:,}")

            # å·²åˆ†ç±»çš„ç»¿è‰²æŠ€æœ¯
            result = session.run("""
                MATCH (p:Patent)-[r:CLASSIFIED_AS]->(g:GreenCategory)
                RETURN count(DISTINCT p) AS classified
            """)
            classified = result.single()['classified']
            print(f"å·²åˆ†ç±»ï¼ˆç»¿è‰²æŠ€æœ¯ï¼‰: {classified:,} ({classified / total * 100:.1f}%)")

            # å·²æå–åœ°ç‚¹
            result = session.run("""
                MATCH (p:Patent)-[r:LOCATED_IN]->(l:Location)
                RETURN count(DISTINCT p) AS with_location
            """)
            with_location = result.single()['with_location']
            print(f"å·²æå–åœ°ç‚¹: {with_location:,} ({with_location / total * 100:.1f}%)")

            # ç›¸ä¼¼å…³ç³»
            result = session.run("""
                MATCH (p1:Patent)-[r:SIMILAR_TO]->(p2:Patent)
                RETURN count(r) AS similarities
            """)
            similarities = result.single()['similarities']
            print(f"ç›¸ä¼¼å…³ç³»: {similarities:,}")

            # æŠ€æœ¯é¢†åŸŸ
            result = session.run("""
                MATCH (p:Patent)-[r:ALSO_BELONGS_TO]->(t:TechDomain)
                RETURN count(DISTINCT p) AS with_tech
            """)
            with_tech = result.single()['with_tech']
            print(f"å·²åˆ†æŠ€æœ¯é¢†åŸŸ: {with_tech:,} ({with_tech / total * 100:.1f}%)")

            return {
                'total': total,
                'classified': classified,
                'with_location': with_location,
                'similarities': similarities,
                'with_tech': with_tech
            }

    def check_progress_files(self):
        """æ£€æŸ¥è¿›åº¦æ–‡ä»¶"""
        print("\n=== è¿›åº¦æ–‡ä»¶æ£€æŸ¥ ===")
        output_dir = 'llm_output_previous'

        if not os.path.exists(output_dir):
            print("âœ— è¾“å‡ºç›®å½•ä¸å­˜åœ¨")
            return

        # æŸ¥æ‰¾è¿›åº¦æ–‡ä»¶
        progress_files = [f for f in os.listdir(output_dir) if f.endswith('_progress.json')]
        print(f"å‘ç° {len(progress_files)} ä¸ªè¿›åº¦æ–‡ä»¶:")

        for pf in progress_files:
            filepath = os.path.join(output_dir, pf)
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                session_id = data['session_id']
                processed = len(data.get('processed_patents', []))
                last_update = data.get('last_update', 'unknown')
                print(f"  - {session_id}: {processed:,} ä¸ªä¸“åˆ©å·²å¤„ç†")
                print(f"    æœ€åæ›´æ–°: {last_update}")

    def sample_results(self, n=5):
        """æŠ½æ ·æ£€æŸ¥ç»“æœè´¨é‡"""
        print(f"\n=== æŠ½æ ·æ£€æŸ¥ï¼ˆ{n}ä¸ªæ ·æœ¬ï¼‰===")

        with self.driver.session() as session:
            # æŠ½å–å·²åˆ†ç±»çš„ä¸“åˆ©
            result = session.run("""
                MATCH (p:Patent)-[r:CLASSIFIED_AS]->(g:GreenCategory)
                RETURN p.patentId AS id,
                       p.titleZh AS title,
                       g.code AS category_code,
                       g.name AS category_name,
                       r.confidence AS confidence,
                       r.reasoning AS reasoning
                ORDER BY rand()
                LIMIT $n
            """, n=n)

            print("\nç»¿è‰²æŠ€æœ¯åˆ†ç±»æ ·æœ¬:")
            for i, record in enumerate(result, 1):
                print(f"\næ ·æœ¬ {i}:")
                print(f"  ä¸“åˆ©ID: {record['id']}")
                print(f"  æ ‡é¢˜: {record['title'][:50]}...")
                print(f"  åˆ†ç±»: {record['category_code']} - {record['category_name']}")
                print(f"  ç½®ä¿¡åº¦: {record['confidence']:.2f}")
                print(f"  ç†ç”±: {record['reasoning'][:100]}...")

    def check_cache_size(self):
        """æ£€æŸ¥ç¼“å­˜å¤§å°"""
        print("\n=== ç¼“å­˜ç»Ÿè®¡ ===")
        cache_dir = 'llm_output_previous/cache'

        if not os.path.exists(cache_dir):
            print("ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
            return

        cache_files = [f for f in os.listdir(cache_dir) if f.endswith('.json')]
        total_size = sum(os.path.getsize(os.path.join(cache_dir, f)) for f in cache_files)

        print(f"ç¼“å­˜æ–‡ä»¶æ•°: {len(cache_files)}")
        print(f"ç¼“å­˜å¤§å°: {total_size / 1024 / 1024:.2f} MB")
        print(f"é¢„ä¼°èŠ‚çœAPIè°ƒç”¨: {len(cache_files):,} æ¬¡")

    def performance_test(self, sample_size=10):
        """æ€§èƒ½æµ‹è¯•"""
        print(f"\n=== æ€§èƒ½æµ‹è¯•ï¼ˆ{sample_size}ä¸ªä¸“åˆ©ï¼‰===")

        # è¿™é‡Œå¯ä»¥è°ƒç”¨ä¼˜åŒ–ç‰ˆçš„ä»£ç è¿›è¡Œå°è§„æ¨¡æµ‹è¯•
        print("æ³¨æ„ï¼šå®Œæ•´æ€§èƒ½æµ‹è¯•éœ€è¦è¿è¡Œ llm_enhancement_optimized.py")
        print(f"å»ºè®®ï¼šå…ˆç”¨ {sample_size} ä¸ªä¸“åˆ©æµ‹è¯•ï¼Œå†æ‰©å¤§è§„æ¨¡")

    def compare_with_original(self, session_id):
        """å¯¹æ¯”åŸç‰ˆå’Œä¼˜åŒ–ç‰ˆçš„ç»“æœ"""
        print(f"\n=== ç»“æœå¯¹æ¯” ===")
        # æ£€æŸ¥æŒ‡å®šä¼šè¯çš„ç»“æœæ–‡ä»¶
        output_dir = 'llm_output_previous'

        # æŸ¥æ‰¾è¯¥ä¼šè¯çš„æ‰€æœ‰ç»“æœæ–‡ä»¶
        files = [f for f in os.listdir(output_dir) if f.startswith(session_id)]

        print(f"ä¼šè¯ {session_id} çš„ç»“æœæ–‡ä»¶:")
        for f in files:
            filepath = os.path.join(output_dir, f)
            if f.endswith('.json') and 'progress' not in f:
                with open(filepath, 'r', encoding='utf-8') as file:
                    data = json.load(file)
                    print(f"  - {f}: {len(data)} æ¡è®°å½•")


def run_quick_test():
    """å¿«é€Ÿæµ‹è¯•"""
    print("=" * 60)
    print("LLMå¢å¼ºæ¨¡å— - å¿«é€Ÿæµ‹è¯•")
    print("=" * 60)

    validator = Validator()

    try:
        # 1. æµ‹è¯•è¿æ¥
        if not validator.test_connection():
            print("\nâš  æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
            return

        # 2. è·å–ç»Ÿè®¡
        stats = validator.get_patent_stats()

        # 3. æ£€æŸ¥è¿›åº¦
        validator.check_progress_files()

        # 4. æ£€æŸ¥ç¼“å­˜
        validator.check_cache_size()

        # 5. æŠ½æ ·æ£€æŸ¥
        if stats['classified'] > 0:
            validator.sample_results(n=3)

        print("\n" + "=" * 60)
        print("âœ“ æµ‹è¯•å®Œæˆ")
        print("=" * 60)

        # ç»™å‡ºå»ºè®®
        print("\nå»ºè®®ï¼š")
        if stats['classified'] < stats['total'] * 0.1:
            print("1. è¿è¡Œ llm_enhancement_optimized.py å¼€å§‹å¤„ç†ä¸“åˆ©")
        else:
            print("1. ç»§ç»­è¿è¡Œ llm_enhancement_optimized.py å¤„ç†æ›´å¤šä¸“åˆ©")

        print("2. å®šæœŸè¿è¡Œæ­¤è„šæœ¬æ£€æŸ¥è¿›åº¦")
        print("3. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ llm_enhancement_log.txt äº†è§£è¯¦æƒ…")

    finally:
        validator.close()


def estimate_completion_time(stats):
    """é¢„ä¼°å®Œæˆæ—¶é—´"""
    if stats['classified'] == 0:
        return

    print("\n=== å®Œæˆæ—¶é—´é¢„ä¼° ===")
    total = stats['total']
    processed = stats['classified']

    if processed < 10:
        print("æ ·æœ¬å¤ªå°ï¼Œæ— æ³•å‡†ç¡®é¢„ä¼°")
        return

    # å‡è®¾å¤„ç†é€Ÿåº¦ï¼šæ¯æ‰¹10ä¸ªä¸“åˆ©ï¼Œ30-40ç§’
    remaining = total - processed
    batches = remaining / 10
    estimated_seconds = batches * 35  # å¹³å‡35ç§’/æ‰¹

    hours = estimated_seconds / 3600
    days = hours / 24

    print(f"å·²å¤„ç†: {processed:,} / {total:,} ({processed / total * 100:.1f}%)")
    print(f"å‰©ä½™: {remaining:,} ä¸ªä¸“åˆ©")
    print(f"é¢„è®¡è¿˜éœ€: {hours:.1f} å°æ—¶ ({days:.1f} å¤©)")

    if hours < 1:
        print("âœ“ å¾ˆå¿«å°±èƒ½å®Œæˆï¼")
    elif hours < 24:
        print("âœ“ ä»Šå¤©æˆ–æ˜å¤©å¯ä»¥å®Œæˆ")
    else:
        print(f"ğŸ’¡ å»ºè®®: åˆ† {int(days) + 1} å¤©å®Œæˆï¼Œæ¯å¤©è¿è¡Œå‡ å°æ—¶")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == '--full':
        # å®Œæ•´æµ‹è¯•
        validator = Validator()
        try:
            validator.test_connection()
            stats = validator.get_patent_stats()
            validator.check_progress_files()
            validator.check_cache_size()
            validator.sample_results(n=10)
            estimate_completion_time(stats)
        finally:
            validator.close()
    else:
        # å¿«é€Ÿæµ‹è¯•
        run_quick_test()