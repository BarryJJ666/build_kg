config.py:
# -*- coding: utf-8 -*-
"""
配置文件
"""
# Neo4j数据库配置
NEO4J_CONFIG = {
    'uri': 'bolt://8.130.70.72:21099',
    'user': 'neo4j',
    'password': 'zyz_password'
}

# LLM配置(OpenAI兼容API)
LLM_CONFIG = {
    'api_base': 'https://ai.gitee.com/v1',
    'api_key': 'BRGYKADH3CKIAGSETETJBD95C1TAKPICNJMY22II',
    'model': 'Qwen2.5-72B-Instruct',
    'temperature': 0.1,
    'max_tokens': 2000,
    'timeout': 30
}

# 数据文件配置
DATA_CONFIG = {
    'data_dir': './data/',
    # 文件名到技术领域代码的映射
    'file_mapping': {
        "2.6.1制氢技术-1.xlsx": "H1.1",
        "2.6.1制氢技术-2.xlsx": "H1.1",
        "2.6.2.1 物理储氢.xlsx": "H2.1",
        "2.6.2.2合金储氢.xlsx": "H2.3",
        "2.6.2.3无机储氢-1.xlsx": "H2.3",
        "2.6.2.3无机储氢-2.xlsx": "H2.3",
        "2.6.2.4有机储氢.xlsx": "H2.4",
        "2.6.3氢燃料电池.xlsx": "H3.1",
        "2.6.4氢制冷.xlsx": "H3.4"
    }
}

# 日志配置
LOG_CONFIG = {
    'level': 'INFO',
    'format': '%(asctime)s - %(levelname)s - %(message)s',
    'file': 'import_log.txt'
}

# 绿色技术分类定义
GREEN_TECH_CATEGORIES = {
    '零碳使能型': {
        'code': 'GT1',
        'description': '直接支撑绿氢经济的核心技术，必须或显著提升可再生能源制氢的可行性，降低绿氢成本、能耗或间歇性障碍，实现全流程近零碳排放。',
        'examples': ['PEM电解槽优化', '风光氢离网耦合控制系统', '低能耗有机液体储氢（LOHC）脱氢技术', '绿氢燃料电池重卡']
    },
    '低碳过渡型': {
        'code': 'GT2',
        'description': '支持蓝氢或显著减碳的过渡性技术，依赖碳捕集与封存（CCS/CCUS）实现大幅减排（建议捕集率≥85%），或用于替代高碳终端应用（如煤制氨转为氢制氨）。',
        'examples': ['蒸汽甲烷重整（SMR）+高效CCS系统', '蓝氢专用输氢管道', '氢基直接还原铁（H₂-DRI）炼钢工艺']
    },
    '绿氢兼容型': {
        'code': 'GT3',
        'description': '技术本身不产生碳排放，可用于绿氢也可用于灰氢，无化石能源依赖，不锁定高碳路径，但也不专属促进零碳转型。',
        'examples': ['高压IV型储氢瓶', '标准加氢站设备', '通用质子交换膜（PEM）', '氢气压缩机']
    },
    '碳锁定型': {
        'code': 'GT4',
        'description': '强化高碳制氢路径，仅优化灰氢或棕氢工艺，延长化石能源制氢寿命，且未集成CCS或可再生电力接口，阻碍系统脱碳。',
        'examples': ['无CCS的SMR催化剂效率提升', '炼油厂专用灰氢提纯装置', '基于天然气管网的高比例灰氢输配系统']
    },
    '中性/模糊型': {
        'code': 'GT5',
        'description': '无法明确判断其绿色贡献的技术，通常为通用基础部件、安全装置或未限定应用场景/能源来源的模块。',
        'examples': ['氢气泄漏传感器', '密封材料改进', '未限定电源类型的电解电源管理模块', '通用阀门或连接件']
    }
}

# LLM增强配置
LLM_ENHANCE_CONFIG = {
    'batch_size': 10,
    'output_dir': './llm_output/',
    'enable_tech_classification': True,
    'enable_green_classification': True,
    'enable_semantic_relations': True,
    'enable_location_extraction': True,
    'max_workers': 3
}




import_patents.py:
# -*- coding: utf-8 -*-
"""
氢能专利知识图谱 - 数据导入模块 (修复版)
主要功能:
1. 创建所有确定的节点: Patent, Entity, Date, Province, City, District, TechDomain
2. 创建所有确定的边关系
3. 批量处理提升性能
4. 地理位置解析(基于规则,不依赖LLM)
"""

import os
import re
import pandas as pd
from datetime import datetime
from neo4j import GraphDatabase
import logging
import argparse
from tech_domains import get_all_tech_domains
from config import NEO4J_CONFIG, DATA_CONFIG, LOG_CONFIG
from concurrent.futures import ThreadPoolExecutor, as_completed

# 配置日志
logging.basicConfig(
    level=getattr(logging, LOG_CONFIG['level']),
    format=LOG_CONFIG['format'],
    handlers=[
        logging.FileHandler(LOG_CONFIG['file'], encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class PatentKnowledgeGraphBuilder:
    """专利知识图谱构建器"""

    def __init__(self, uri, user, password, batch_size=5000):
        """初始化Neo4j连接"""
        self.driver = GraphDatabase.driver(
            uri,
            auth=(user, password),
            max_connection_pool_size=100,
            connection_timeout=60
        )
        self.batch_size = batch_size

        # 中国省份、直辖市、自治区列表
        self.provinces = {
            '北京': '北京市', '天津': '天津市', '上海': '上海市', '重庆': '重庆市',
            '河北': '河北省', '山西': '山西省', '辽宁': '辽宁省', '吉林': '吉林省',
            '黑龙江': '黑龙江省', '江苏': '江苏省', '浙江': '浙江省', '安徽': '安徽省',
            '福建': '福建省', '江西': '江西省', '山东': '山东省', '河南': '河南省',
            '湖北': '湖北省', '湖南': '湖南省', '广东': '广东省', '海南': '海南省',
            '四川': '四川省', '贵州': '贵州省', '云南': '云南省', '陕西': '陕西省',
            '甘肃': '甘肃省', '青海': '青海省', '台湾': '台湾省',
            '内蒙古': '内蒙古自治区', '广西': '广西壮族自治区', '西藏': '西藏自治区',
            '宁夏': '宁夏回族自治区', '新疆': '新疆维吾尔自治区',
            '香港': '香港特别行政区', '澳门': '澳门特别行政区'
        }

        logger.info(f"Connected to Neo4j at {uri}")

    def close(self):
        """关闭连接"""
        self.driver.close()
        logger.info("Neo4j connection closed")

    def clear_database(self):
        """清空数据库"""
        logger.warning("Clearing database...")
        with self.driver.session() as session:
            session.run("MATCH (n) DETACH DELETE n")
            logger.info("Database cleared successfully")

    def create_constraints(self):
        """创建约束"""
        logger.info("Creating constraints...")
        with self.driver.session() as session:
            constraints = [
                "CREATE CONSTRAINT patent_id IF NOT EXISTS FOR (p:Patent) REQUIRE p.patentId IS UNIQUE",
                "CREATE CONSTRAINT entity_normalized_name IF NOT EXISTS FOR (e:Entity) REQUIRE e.normalizedName IS UNIQUE",
                "CREATE CONSTRAINT techDomain_code IF NOT EXISTS FOR (t:TechDomain) REQUIRE t.code IS UNIQUE",
                "CREATE CONSTRAINT date_value IF NOT EXISTS FOR (d:Date) REQUIRE d.dateValue IS UNIQUE",
                "CREATE CONSTRAINT province_name IF NOT EXISTS FOR (p:Province) REQUIRE p.name IS UNIQUE",
                "CREATE CONSTRAINT city_name IF NOT EXISTS FOR (c:City) REQUIRE c.fullName IS UNIQUE",
                "CREATE CONSTRAINT district_name IF NOT EXISTS FOR (d:District) REQUIRE d.fullName IS UNIQUE"
            ]

            for constraint in constraints:
                try:
                    session.run(constraint)
                    logger.info(f"✓ {constraint.split('CONSTRAINT')[1].split('IF')[0].strip()}")
                except Exception as e:
                    logger.debug(f"Constraint exists: {e}")

    def create_indexes(self):
        """创建索引(导入完成后)"""
        logger.info("Creating indexes...")
        with self.driver.session() as session:
            indexes = [
                "CREATE INDEX patent_pubNumber IF NOT EXISTS FOR (p:Patent) ON (p.pubNumber)",
                "CREATE INDEX patent_appNumber IF NOT EXISTS FOR (p:Patent) ON (p.appNumber)",
                "CREATE INDEX patent_titleZh IF NOT EXISTS FOR (p:Patent) ON (p.titleZh)",
                "CREATE INDEX entity_type IF NOT EXISTS FOR (e:Entity) ON (e.type)",
                "CREATE INDEX entity_name IF NOT EXISTS FOR (e:Entity) ON (e.name)",
                "CREATE INDEX province_name_idx IF NOT EXISTS FOR (p:Province) ON (p.name)",
                "CREATE INDEX city_name_idx IF NOT EXISTS FOR (c:City) ON (c.name)"
            ]

            for index in indexes:
                try:
                    session.run(index)
                    logger.info(f"✓ {index.split('INDEX')[1].split('IF')[0].strip()}")
                except Exception as e:
                    logger.debug(f"Index exists: {e}")

    def create_tech_domains(self):
        """创建技术领域树"""
        logger.info("Creating technology domain tree...")
        domains = get_all_tech_domains()

        with self.driver.session() as session:
            query = """
            UNWIND $domains AS domain
            MERGE (t:TechDomain {code: domain.code})
            SET t.nameZh = domain.nameZh,
                t.nameEn = domain.nameEn,
                t.level = domain.level
            """
            session.run(query, domains=domains)

            parent_rels = [{'child_code': d['code'], 'parent_code': d['parent_code']}
                           for d in domains if d['parent_code']]

            if parent_rels:
                query = """
                UNWIND $rels AS rel
                MATCH (child:TechDomain {code: rel.child_code})
                MATCH (parent:TechDomain {code: rel.parent_code})
                MERGE (child)-[:SUBDOMAIN_OF]->(parent)
                """
                session.run(query, rels=parent_rels)

            logger.info(f"✓ Created {len(domains)} technology domains")

    def extract_location_from_entity(self, entity_name):
        """
        从实体名称中提取地理位置信息 (基于规则,不使用LLM)
        返回: {'province': 'XX', 'city': 'XX', 'district': 'XX'}
        """
        if not entity_name or len(entity_name) < 2:
            return None

        result = {'province': None, 'city': None, 'district': None}

        # 检查省份
        for short_name, full_name in self.provinces.items():
            if short_name in entity_name:
                result['province'] = full_name

                # 尝试提取市级信息
                # 匹配模式: 省份名 + 可选的"省" + 城市名 + "市"
                city_pattern = rf'{short_name}[省市]?([^省市县区]{2, 8})市'
                city_match = re.search(city_pattern, entity_name)
                if city_match:
                    result['city'] = city_match.group(1) + '市'

                    # 尝试提取区县信息
                    district_pattern = rf"{city_match.group(1)}市([^市县区]{2, 6})[县区]"
                    district_match = re.search(district_pattern, entity_name)
                    if district_match:
                        result['district'] = district_match.group(1)
                        # 判断是县还是区
                        if '县' in entity_name[district_match.end() - 1:district_match.end() + 1]:
                            result['district'] += '县'
                        else:
                            result['district'] += '区'

                break

        # 如果找到了至少省份信息,返回结果
        if result['province']:
            return result

        return None

    def normalize_patent_number(self, number_str):
        """规范化专利号"""
        if pd.isna(number_str) or not number_str:
            return None
        normalized = str(number_str).strip().replace(' ', '').replace('\n', '')
        return normalized if normalized else None

    def normalize_entity_name(self, name_str):
        """规范化实体名称"""
        if pd.isna(name_str) or not name_str:
            return None

        name = str(name_str).strip()
        if len(name) < 2:
            return None

        # 移除常见公司后缀
        common_suffixes = [
            '有限责任公司', '股份有限公司', '有限公司',
            'Co.,Ltd.', 'Co., Ltd.', 'Co.,Ltd', 'Co., Ltd',
            'Ltd.', 'Ltd', 'Inc.', 'Inc', 'Corp.', 'Corp',
            'LIMITED', 'CORPORATION', 'INCORPORATED'
        ]

        original_name = name
        for suffix in common_suffixes:
            if name.endswith(suffix):
                name = name[:-len(suffix)].strip()
                break

        if not name or len(name) < 2:
            return original_name

        # 移除多余的空格和标点
        name = re.sub(r'\s+', '', name)
        name = re.sub(r'[（）\(\)【】\[\]《》<>]', '', name)

        return name if name and len(name) >= 2 else original_name

    def parse_date(self, date_str):
        """解析日期为YYYY-MM-DD格式"""
        if pd.isna(date_str) or not date_str:
            return None

        date_str = str(date_str).strip()
        formats = ['%Y-%m-%d', '%Y/%m/%d', '%Y.%m.%d', '%Y%m%d',
                   '%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S']

        for fmt in formats:
            try:
                return datetime.strptime(date_str, fmt).strftime('%Y-%m-%d')
            except ValueError:
                continue
        return None

    def split_entities(self, entity_str):
        """分割实体字符串"""
        if pd.isna(entity_str) or not entity_str:
            return []
        entity_str = str(entity_str).strip()
        entities = re.split(r'[;,、；]', entity_str)
        return [e.strip() for e in entities if e.strip() and len(e.strip()) >= 2]

    def process_file_data(self, file_path, tech_domain_code):
        """处理单个文件,返回结构化数据"""
        logger.info(f"Processing: {os.path.basename(file_path)}")

        df = pd.read_excel(file_path)
        patents_list = []
        stats = {'total': 0, 'valid': 0, 'skipped': 0}

        for idx, row in df.iterrows():
            stats['total'] += 1

            pub_number = self.normalize_patent_number(row.get('公开(公告)号'))
            app_number = self.normalize_patent_number(row.get('申请号'))

            if not pub_number and not app_number:
                stats['skipped'] += 1
                continue

            patent_id = pub_number if pub_number else app_number

            patent_data = {
                'patentId': patent_id,
                'pubNumber': pub_number,
                'appNumber': app_number,
                'titleZh': str(row.get('标题 (中文)', '')).strip() or None,
                'titleEn': str(row.get('标题 (英文)', '')).strip() or None,
                'abstractZh': str(row.get('摘要 (中文)', '')).strip() or None,
                'abstractEn': str(row.get('摘要 (英文)', '')).strip() or None,
                'patentType': str(row.get('专利类型', '')).strip() or None,
                'legalStatus': str(row.get('当前法律状态', '')).strip() or None,
                'ipcMainClass': str(row.get('IPC主分类', '')).strip() or None,
                'publicCountry': str(row.get('公开国别', '')).strip() or None,
                'techDomainCode': tech_domain_code,

                'applicants': self.split_entities(row.get('申请人')),
                'currentOwners': self.split_entities(row.get('当前权利人')),
                'appDate': self.parse_date(row.get('申请日')),
                'pubDate': self.parse_date(
                    row.get('公开(公告)日\n(如专利类型为"发明授权"或"实用新型"或"外观设计"的,此项为授权公告日)')),
                'familyPatents': self.split_entities(row.get('简单同族')),
                'assignors': self.split_entities(row.get('转让人')),
                'assignees': self.split_entities(row.get('受让人')),
                'licensors': self.split_entities(row.get('许可人')),
                'currentLicensees': self.split_entities(row.get('当前被许可人')),
                'pledgors': self.split_entities(row.get('出质人')),
                'pledgees': self.split_entities(row.get('质权人')),
                'plaintiffs': self.split_entities(row.get('原告')),
                'defendants': self.split_entities(row.get('被告'))
            }

            patents_list.append(patent_data)
            stats['valid'] += 1

        logger.info(f"✓ Processed {os.path.basename(file_path)}: "
                    f"{stats['valid']}/{stats['total']} valid patents, "
                    f"{stats['skipped']} skipped")
        return patents_list

    def batch_create_all_nodes_and_relationships(self, patents_batch):
        """批量创建所有节点和关系"""
        with self.driver.session() as session:
            # 准备批量数据
            batch_data = []

            for patent in patents_batch:
                item = {
                    'patent': {
                        'patentId': patent['patentId'],
                        'pubNumber': patent['pubNumber'],
                        'appNumber': patent['appNumber'],
                        'titleZh': patent['titleZh'],
                        'titleEn': patent['titleEn'],
                        'abstractZh': patent['abstractZh'],
                        'abstractEn': patent['abstractEn'],
                        'patentType': patent['patentType'],
                        'legalStatus': patent['legalStatus'],
                        'ipcMainClass': patent['ipcMainClass'],
                        'publicCountry': patent['publicCountry'],
                        'techDomainCode': patent['techDomainCode']
                    },
                    'dates': [],
                    'entities': [],
                    'familyPatents': patent.get('familyPatents', [])
                }

                # 添加日期信息
                if patent['appDate']:
                    item['dates'].append({
                        'dateValue': patent['appDate'],
                        'dateType': '申请日',
                        'relType': 'FILED_ON'
                    })
                if patent['pubDate']:
                    item['dates'].append({
                        'dateValue': patent['pubDate'],
                        'dateType': '公开公告日',
                        'relType': 'PUBLISHED_ON'
                    })

                # 添加实体信息
                for applicant in patent.get('applicants', []):
                    normalized = self.normalize_entity_name(applicant)
                    if normalized:
                        location = self.extract_location_from_entity(applicant)
                        item['entities'].append({
                            'originalName': applicant,
                            'normalizedName': normalized,
                            'relType': 'APPLIED_BY',
                            'location': location
                        })

                for owner in patent.get('currentOwners', []):
                    normalized = self.normalize_entity_name(owner)
                    if normalized:
                        location = self.extract_location_from_entity(owner)
                        item['entities'].append({
                            'originalName': owner,
                            'normalizedName': normalized,
                            'relType': 'OWNED_BY',
                            'location': location
                        })

                for assignor in patent.get('assignors', []):
                    normalized = self.normalize_entity_name(assignor)
                    if normalized:
                        item['entities'].append({
                            'originalName': assignor,
                            'normalizedName': normalized,
                            'relType': 'ASSIGNED_FROM',
                            'location': None
                        })

                for assignee in patent.get('assignees', []):
                    normalized = self.normalize_entity_name(assignee)
                    if normalized:
                        item['entities'].append({
                            'originalName': assignee,
                            'normalizedName': normalized,
                            'relType': 'ASSIGNED_TO',
                            'location': None
                        })

                for licensee in patent.get('currentLicensees', []):
                    normalized = self.normalize_entity_name(licensee)
                    if normalized:
                        item['entities'].append({
                            'originalName': licensee,
                            'normalizedName': normalized,
                            'relType': 'LICENSED_TO',
                            'location': None
                        })

                for pledgor in patent.get('pledgors', []):
                    normalized = self.normalize_entity_name(pledgor)
                    if normalized:
                        item['entities'].append({
                            'originalName': pledgor,
                            'normalizedName': normalized,
                            'relType': 'PLEDGED_BY',
                            'location': None
                        })

                for pledgee in patent.get('pledgees', []):
                    normalized = self.normalize_entity_name(pledgee)
                    if normalized:
                        item['entities'].append({
                            'originalName': pledgee,
                            'normalizedName': normalized,
                            'relType': 'PLEDGED_TO',
                            'location': None
                        })

                for plaintiff in patent.get('plaintiffs', []):
                    normalized = self.normalize_entity_name(plaintiff)
                    if normalized:
                        item['entities'].append({
                            'originalName': plaintiff,
                            'normalizedName': normalized,
                            'relType': 'PLAINTIFF_IN',
                            'location': None
                        })

                for defendant in patent.get('defendants', []):
                    normalized = self.normalize_entity_name(defendant)
                    if normalized:
                        item['entities'].append({
                            'originalName': defendant,
                            'normalizedName': normalized,
                            'relType': 'DEFENDANT_IN',
                            'location': None
                        })

                batch_data.append(item)

            # 执行批量创建
            query = """
            UNWIND $batch AS item

            // 创建专利节点
            MERGE (patent:Patent {patentId: item.patent.patentId})
            SET patent.pubNumber = item.patent.pubNumber,
                patent.appNumber = item.patent.appNumber,
                patent.titleZh = item.patent.titleZh,
                patent.titleEn = item.patent.titleEn,
                patent.abstractZh = item.patent.abstractZh,
                patent.abstractEn = item.patent.abstractEn,
                patent.patentType = item.patent.patentType,
                patent.legalStatus = item.patent.legalStatus,
                patent.ipcMainClass = item.patent.ipcMainClass,
                patent.publicCountry = item.patent.publicCountry

            // 技术领域关系
            WITH patent, item
            WHERE item.patent.techDomainCode IS NOT NULL
            MATCH (t:TechDomain {code: item.patent.techDomainCode})
            MERGE (patent)-[:BELONGS_TO_TECH]->(t)

            // 日期节点和关系
            WITH patent, item
            UNWIND item.dates AS dateInfo
            MERGE (d:Date {dateValue: dateInfo.dateValue})
            SET d.dateType = dateInfo.dateType
            WITH patent, d, dateInfo
            CALL apoc.create.relationship(patent, dateInfo.relType, {}, d) YIELD rel

            RETURN count(patent) as created
            """

            # 如果没有APOC,使用标准Cypher
            query_no_apoc = """
            UNWIND $batch AS item

            // 创建专利节点
            MERGE (patent:Patent {patentId: item.patent.patentId})
            SET patent.pubNumber = item.patent.pubNumber,
                patent.appNumber = item.patent.appNumber,
                patent.titleZh = item.patent.titleZh,
                patent.titleEn = item.patent.titleEn,
                patent.abstractZh = item.patent.abstractZh,
                patent.abstractEn = item.patent.abstractEn,
                patent.patentType = item.patent.patentType,
                patent.legalStatus = item.patent.legalStatus,
                patent.ipcMainClass = item.patent.ipcMainClass,
                patent.publicCountry = item.patent.publicCountry

            // 技术领域关系
            WITH patent, item
            WHERE item.patent.techDomainCode IS NOT NULL
            MATCH (t:TechDomain {code: item.patent.techDomainCode})
            MERGE (patent)-[:BELONGS_TO_TECH]->(t)

            RETURN count(patent) as created
            """

            session.run(query_no_apoc, batch=batch_data)

            # ========== 批量创建日期节点和关系 ==========
            date_batch = []
            for item in batch_data:
                for dateInfo in item['dates']:
                    date_batch.append({
                        'patentId': item['patent']['patentId'],
                        'dateValue': dateInfo['dateValue'],
                        'dateType': dateInfo['dateType'],
                        'relType': dateInfo['relType']
                    })

            if date_batch:
                # 批量创建申请日关系
                filed_dates = [d for d in date_batch if d['relType'] == 'FILED_ON']
                if filed_dates:
                    session.run("""
                        UNWIND $batch AS item
                        MATCH (p:Patent {patentId: item.patentId})
                        MERGE (d:Date {dateValue: item.dateValue})
                        SET d.dateType = item.dateType
                        MERGE (p)-[:FILED_ON]->(d)
                    """, batch=filed_dates)

                # 批量创建公开日关系
                pub_dates = [d for d in date_batch if d['relType'] == 'PUBLISHED_ON']
                if pub_dates:
                    session.run("""
                        UNWIND $batch AS item
                        MATCH (p:Patent {patentId: item.patentId})
                        MERGE (d:Date {dateValue: item.dateValue})
                        SET d.dateType = item.dateType
                        MERGE (p)-[:PUBLISHED_ON]->(d)
                    """, batch=pub_dates)

            # ========== 批量创建实体节点 ==========
            entity_batch = []
            for item in batch_data:
                for entity_info in item['entities']:
                    entity_batch.append({
                        'normalizedName': entity_info['normalizedName'],
                        'originalName': entity_info['originalName']
                    })

            if entity_batch:
                # 去重
                unique_entities = {e['normalizedName']: e for e in entity_batch}.values()
                session.run("""
                    UNWIND $batch AS item
                    MERGE (e:Entity {normalizedName: item.normalizedName})
                    SET e.name = item.originalName,
                        e.type = CASE
                            WHEN item.originalName =~ '.*公司.*|.*Corp.*|.*Ltd.*|.*Inc.*' THEN '企业'
                            ELSE '个人/其他'
                        END
                """, batch=list(unique_entities))

            # ========== 批量创建专利-实体关系 ==========
            # 按关系类型分组
            rel_by_type = {}
            for item in batch_data:
                for entity_info in item['entities']:
                    rel_type = entity_info['relType']
                    if rel_type not in rel_by_type:
                        rel_by_type[rel_type] = []
                    rel_by_type[rel_type].append({
                        'patentId': item['patent']['patentId'],
                        'normalizedName': entity_info['normalizedName']
                    })

            # 批量创建每种类型的关系
            for rel_type, rels in rel_by_type.items():
                if rels:
                    session.run(f"""
                        UNWIND $batch AS item
                        MATCH (p:Patent {{patentId: item.patentId}})
                        MATCH (e:Entity {{normalizedName: item.normalizedName}})
                        MERGE (p)-[:{rel_type}]->(e)
                    """, batch=rels)

            # ========== 批量创建地理节点和关系 ==========
            province_batch = []
            city_batch = []
            district_batch = []

            for item in batch_data:
                for entity_info in item['entities']:
                    if entity_info['location']:
                        loc = entity_info['location']

                        # 收集省份数据
                        if loc['province']:
                            province_batch.append({
                                'normalizedName': entity_info['normalizedName'],
                                'provinceName': loc['province']
                            })

                        # 收集城市数据
                        if loc['city'] and loc['province']:
                            city_batch.append({
                                'normalizedName': entity_info['normalizedName'],
                                'fullName': f"{loc['province']}-{loc['city']}",
                                'cityName': loc['city'],
                                'provinceName': loc['province']
                            })

                        # 收集区县数据
                        if loc['district'] and loc['city'] and loc['province']:
                            district_batch.append({
                                'normalizedName': entity_info['normalizedName'],
                                'fullName': f"{loc['province']}-{loc['city']}-{loc['district']}",
                                'districtName': loc['district'],
                                'cityName': loc['city'],
                                'provinceName': loc['province']
                            })

            # 批量创建省份节点和关系
            if province_batch:
                session.run("""
                    UNWIND $batch AS item
                    MERGE (p:Province {name: item.provinceName})
                """, batch=province_batch)

                session.run("""
                    UNWIND $batch AS item
                    MATCH (e:Entity {normalizedName: item.normalizedName})
                    MATCH (p:Province {name: item.provinceName})
                    MERGE (e)-[:LOCATED_IN_PROVINCE]->(p)
                """, batch=province_batch)

            # 批量创建城市节点和关系
            if city_batch:
                session.run("""
                    UNWIND $batch AS item
                    MERGE (c:City {fullName: item.fullName})
                    SET c.name = item.cityName,
                        c.province = item.provinceName
                """, batch=city_batch)

                session.run("""
                    UNWIND $batch AS item
                    MATCH (e:Entity {normalizedName: item.normalizedName})
                    MATCH (c:City {fullName: item.fullName})
                    MERGE (e)-[:LOCATED_IN_CITY]->(c)
                """, batch=city_batch)

            # 批量创建区县节点和关系
            if district_batch:
                session.run("""
                    UNWIND $batch AS item
                    MERGE (d:District {fullName: item.fullName})
                    SET d.name = item.districtName,
                        d.city = item.cityName,
                        d.province = item.provinceName
                """, batch=district_batch)

                session.run("""
                    UNWIND $batch AS item
                    MATCH (e:Entity {normalizedName: item.normalizedName})
                    MATCH (d:District {fullName: item.fullName})
                    MERGE (e)-[:LOCATED_IN_DISTRICT]->(d)
                """, batch=district_batch)

            # ========== 批量创建同族专利关系 ==========
            family_batch = []
            for item in batch_data:
                if item['familyPatents']:
                    for family_patent in item['familyPatents']:
                        family_patent_normalized = self.normalize_patent_number(family_patent)
                        if family_patent_normalized:
                            family_batch.append({
                                'patent1': item['patent']['patentId'],
                                'patent2': family_patent_normalized
                            })

            if family_batch:
                session.run("""
                    UNWIND $batch AS item
                    MATCH (p1:Patent {patentId: item.patent1})
                    MERGE (p2:Patent {patentId: item.patent2})
                    MERGE (p1)-[:HAS_FAMILY]-(p2)
                """, batch=family_batch)

    def import_all_data(self):
        """导入所有数据"""
        logger.info("Starting data import...")

        data_dir = DATA_CONFIG['data_dir']
        file_mapping = DATA_CONFIG['file_mapping']

        all_patents = []

        # 处理所有文件
        for filename, tech_code in file_mapping.items():
            file_path = os.path.join(data_dir, filename)
            if os.path.exists(file_path):
                patents = self.process_file_data(file_path, tech_code)
                all_patents.extend(patents)
            else:
                logger.warning(f"File not found: {file_path}")

        logger.info(f"Total patents to import: {len(all_patents)}")

        # 分批导入
        total_batches = (len(all_patents) + self.batch_size - 1) // self.batch_size

        for i in range(0, len(all_patents), self.batch_size):
            batch = all_patents[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1

            logger.info(f"Processing batch {batch_num}/{total_batches} ({len(batch)} patents)...")
            self.batch_create_all_nodes_and_relationships(batch)

        logger.info(f"✓ Imported {len(all_patents)} patents successfully")


def main():
    """主函数"""
    parser = argparse.ArgumentParser(description='Import patent data into Neo4j knowledge graph')
    parser.add_argument('--clear', action='store_true', help='Clear database before import')
    args = parser.parse_args()

    builder = PatentKnowledgeGraphBuilder(
        NEO4J_CONFIG['uri'],
        NEO4J_CONFIG['user'],
        NEO4J_CONFIG['password'],
        batch_size=5000
    )

    try:
        if args.clear:
            builder.clear_database()

        logger.info("=" * 60)
        logger.info("Starting Knowledge Graph Construction")
        logger.info("=" * 60)

        # Step 1: 创建约束
        builder.create_constraints()

        # Step 2: 创建技术领域树
        builder.create_tech_domains()

        # Step 3: 导入专利数据
        start_time = datetime.now()
        builder.import_all_data()
        end_time = datetime.now()

        logger.info(f"Import time: {(end_time - start_time).total_seconds():.2f} seconds")

        # Step 4: 创建索引
        builder.create_indexes()

        logger.info("=" * 60)
        logger.info("✓ Knowledge Graph Construction Completed!")
        logger.info("✓ Next step: run llm_enhancement.py for LLM-based enhancements")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"Error: {e}", exc_info=True)
        raise
    finally:
        builder.close()


if __name__ == "__main__":
    main()




llm_generate_json.py:
# -*- coding: utf-8 -*-
"""
LLM数据生成模块 - 批量保存到JSON (改进版 - 真正的断点续传)
功能:
1. 使用LLM分析专利数据
2. 将结果实时批量保存到JSON文件
3. 自动检测并恢复最新会话
4. 线程安全的增量写入
5. 每个小批次完成后立即保存进度
"""

import os
import json
import logging
from datetime import datetime
from neo4j import GraphDatabase
from openai import OpenAI
from config import NEO4J_CONFIG, LLM_CONFIG, LLM_ENHANCE_CONFIG, GREEN_TECH_CATEGORIES
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import signal
import sys
from typing import List, Dict, Optional
import threading

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('llm_generate_json.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class JSONStorage:
    """JSON文件存储管理器 - 支持断点续传和实时保存"""

    def __init__(self, output_dir: str, session_id: str):
        self.output_dir = output_dir
        self.session_id = session_id
        os.makedirs(output_dir, exist_ok=True)

        # 各类数据的JSON文件路径
        self.files = {
            'green_classifications': os.path.join(output_dir, f'{session_id}_green_classifications.json'),
            'tech_classifications': os.path.join(output_dir, f'{session_id}_tech_classifications.json'),
            'tech_similarities': os.path.join(output_dir, f'{session_id}_tech_similarities.json'),
            'location_data': os.path.join(output_dir, f'{session_id}_location_data.json'),
            'progress': os.path.join(output_dir, f'{session_id}_progress.json')
        }

        self.lock = threading.Lock()

        # 加载已有数据
        self.data = {
            'green_classifications': self._load_json('green_classifications'),
            'tech_classifications': self._load_json('tech_classifications'),
            'tech_similarities': self._load_json('tech_similarities'),
            'location_data': self._load_json('location_data'),
            'progress': self._load_json('progress') or {'processed_patents': [], 'session_id': session_id}
        }

    def _load_json(self, data_type: str):
        """加载JSON文件"""
        filepath = self.files[data_type]
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if data_type == 'progress':
                        logger.info(f"✓ 已加载进度: {len(data.get('processed_patents', []))} 个专利已处理")
                    else:
                        logger.info(f"✓ 已加载 {data_type}: {len(data) if isinstance(data, list) else 'N/A'} 条记录")
                    return data
            except Exception as e:
                logger.warning(f"加载 {data_type} 失败: {e}")
                if data_type == 'progress':
                    return {'processed_patents': [], 'session_id': self.session_id}
                return []
        return [] if data_type != 'progress' else {'processed_patents': [], 'session_id': self.session_id}

    def _save_json(self, data_type: str):
        """保存JSON文件 - 使用原子操作"""
        filepath = self.files[data_type]
        try:
            # 先写入临时文件,再重命名(原子操作,防止写入过程中断导致文件损坏)
            temp_filepath = filepath + '.tmp'
            with open(temp_filepath, 'w', encoding='utf-8') as f:
                json.dump(self.data[data_type], f, ensure_ascii=False, indent=2)

            # 重命名
            if os.path.exists(filepath):
                os.remove(filepath)
            os.rename(temp_filepath, filepath)

        except Exception as e:
            logger.error(f"保存 {data_type} 失败: {e}")

    def append_data(self, data_type: str, items: List[Dict]):
        """追加数据到JSON文件并立即保存"""
        with self.lock:
            if data_type == 'progress':
                # progress特殊处理
                self.data[data_type]['processed_patents'].extend(items)
            else:
                self.data[data_type].extend(items)
            self._save_json(data_type)

    def mark_processed(self, patent_ids: List[str]):
        """标记专利为已处理并立即保存"""
        with self.lock:
            self.data['progress']['processed_patents'].extend(patent_ids)
            self.data['progress']['last_update'] = datetime.now().isoformat()
            self._save_json('progress')

    def is_processed(self, patent_id: str) -> bool:
        """检查专利是否已处理"""
        return patent_id in self.data['progress']['processed_patents']

    def get_processed_count(self) -> int:
        """获取已处理的专利数量"""
        return len(self.data['progress']['processed_patents'])

    def get_stats(self) -> Dict:
        """获取统计信息"""
        return {
            'session_id': self.session_id,
            'processed_patents': len(self.data['progress']['processed_patents']),
            'green_classifications': len(self.data['green_classifications']),
            'tech_classifications': len(self.data['tech_classifications']),
            'tech_similarities': len(self.data['tech_similarities']),
            'location_data': len(self.data['location_data'])
        }


class LLMDataGenerator:
    """LLM数据生成器 - 支持实时保存和断点续传"""

    def __init__(self, session_id: Optional[str] = None):
        """初始化"""
        # Neo4j连接(仅用于读取专利数据)
        self.driver = GraphDatabase.driver(
            NEO4J_CONFIG['uri'],
            auth=(NEO4J_CONFIG['user'], NEO4J_CONFIG['password']),
            max_connection_lifetime=3600,
            max_connection_pool_size=50,
            keep_alive=True
        )

        # OpenAI客户端
        self.client = OpenAI(
            api_key=LLM_CONFIG['api_key'],
            base_url=LLM_CONFIG['api_base'],
            timeout=60.0,
            max_retries=3
        )

        # 会话ID
        if session_id:
            self.session_id = session_id
            logger.info(f"恢复会话: {self.session_id}")
        else:
            self.session_id = datetime.now().strftime('%Y%m%d_%H%M%S')
            logger.info(f"新建会话: {self.session_id}")

        # JSON存储
        self.storage = JSONStorage(LLM_ENHANCE_CONFIG['output_dir'], self.session_id)

        # 信号处理标志
        self.should_stop = False
        self._setup_signal_handlers()

    def _setup_signal_handlers(self):
        """设置信号处理器"""

        def signal_handler(signum, frame):
            logger.info("\n⚠️ 接收到中断信号，正在安全退出...")
            self.should_stop = True

        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)

    def close(self):
        """关闭连接"""
        self.driver.close()

    def call_llm(self, prompt: str, max_retries: int = 3) -> Optional[str]:
        """调用LLM API"""
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=LLM_CONFIG['model'],
                    messages=[
                        {
                            "role": "system",
                            "content": "你是专利分析专家。仔细分析输入,返回准确的JSON格式结果。"
                        },
                        {"role": "user", "content": prompt}
                    ],
                    temperature=LLM_CONFIG['temperature'],
                    max_tokens=LLM_CONFIG['max_tokens']
                )

                return response.choices[0].message.content.strip()

            except Exception as e:
                logger.warning(f"LLM调用失败 (尝试 {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # 指数退避
                else:
                    logger.error(f"LLM调用失败,已重试{max_retries}次")
                    return None

    def get_unprocessed_patents(self, limit: int = 100) -> List[Dict]:
        """获取未处理的专利"""
        with self.driver.session() as session:
            query = """
            MATCH (p:Patent)
            WHERE p.titleZh IS NOT NULL AND p.abstractZh IS NOT NULL
            RETURN p.patentId AS patentId,
                   p.titleZh AS titleZh,
                   p.abstractZh AS abstractZh,
                   p.titleEn AS titleEn,
                   p.abstractEn AS abstractEn,
                   p.ipcMainClass AS ipcMainClass,
                   p.patentType AS patentType
            ORDER BY p.patentId
            """
            result = session.run(query)

            patents = []
            for record in result:
                patent = dict(record)
                if not self.storage.is_processed(patent['patentId']):
                    patents.append(patent)
                    if len(patents) >= limit:
                        break

            return patents

    def get_total_patents_count(self) -> int:
        """获取专利总数"""
        with self.driver.session() as session:
            result = session.run(
                "MATCH (p:Patent) WHERE p.titleZh IS NOT NULL RETURN count(p) AS cnt"
            )
            return result.single()['cnt']

    def classify_green_technology(self, patents: List[Dict]) -> List[Dict]:
        """分类绿色技术"""
        # 构建分类说明
        categories_desc = []
        for name, data in GREEN_TECH_CATEGORIES.items():
            examples_str = "\n      ".join([f"• {ex}" for ex in data['examples']])
            categories_desc.append(
                f"**{name}** (代码: {data['code']})\n"
                f"  定义: {data['description']}\n"
                f"  典型示例:\n      {examples_str}"
            )

        categories_text = "\n\n".join(categories_desc)

        # 构建专利信息(每批最多5个)
        patents_info = []
        for i, p in enumerate(patents[:5]):
            patents_info.append(
                f"### 专利{i + 1}\n"
                f"ID: {p['patentId']}\n"
                f"标题: {p['titleZh']}\n"
                f"摘要: {p['abstractZh'][:300]}\n"
                f"IPC: {p.get('ipcMainClass', '未知')}"
            )

        prompt = f"""请对以下氢能专利进行绿色技术分类。

{chr(10).join(patents_info)}

## 分类标准

{categories_text}

## 分析要点

1. 识别核心技术路径(电解制氢/化石能源制氢/储运/应用)
2. 判断能源来源依赖(可再生能源/化石能源/通用)
3. 评估碳排放特征(零碳/低碳/高碳/中性)
4. 确定是否锁定特定路径
5. 综合判断所属类别

## 输出格式

返回JSON数组,每个专利一个对象:
```json
[
  {{
    "patent_id": "专利ID",
    "category_code": "GT1",
    "category_name": "零碳使能型",
    "confidence": 0.9,
    "reasoning": "详细的分类理由"
  }}
]
```

只返回JSON,不要其他内容。"""

        response = self.call_llm(prompt)
        if not response:
            return []

        try:
            # 提取JSON
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                results = json.loads(json_str)
                return results if isinstance(results, list) else []
        except json.JSONDecodeError as e:
            logger.warning(f"解析绿色分类结果失败: {e}")
            return []

    def classify_tech_domain(self, patents: List[Dict]) -> List[Dict]:
        """细化技术领域分类"""
        patents_info = []
        for i, p in enumerate(patents[:5]):
            patents_info.append(
                f"### 专利{i + 1}\n"
                f"ID: {p['patentId']}\n"
                f"标题: {p['titleZh']}\n"
                f"摘要: {p['abstractZh'][:300]}\n"
                f"IPC: {p.get('ipcMainClass', '未知')}"
            )

        prompt = f"""请为以下氢能专利识别其所属的技术领域(可能多个)。

{chr(10).join(patents_info)}

## 技术领域代码体系

### H1 制氢
- H1.1 电解水制氢 (H1.1.1碱性电解, H1.1.2质子交换膜电解, H1.1.3固体氧化物电解, H1.1.4阴离子交换膜电解)
- H1.2 化石能源制氢 (H1.2.1天然气重整, H1.2.2煤制氢, H1.2.3甲醇裂解)
- H1.3 可再生能源耦合制氢 (H1.3.1风光电制氢, H1.3.2离网制氢, H1.3.3协同控制)
- H1.4 光/热/生物制氢 (H1.4.1光催化, H1.4.2热化学, H1.4.3生物质)

### H2 储运氢
- H2.1 高压气态储运 (H2.1.1 35MPa, H2.1.2 70MPa, H2.1.3管束车)
- H2.2 低温液态储运 (H2.2.1液氢储罐, H2.2.2加注技术, H2.2.3蒸发控制)
- H2.3 固态储氢 (H2.3.1金属氢化物, H2.3.2化学氢化物, H2.3.3多孔材料)
- H2.4 有机载体储运 (H2.4.1甲苯体系, H2.4.2杂环体系, H2.4.3催化剂)
- H2.5 氢输配基础设施 (H2.5.1管道, H2.5.2加氢站, H2.5.3压缩纯化)

### H3 用氢
- H3.1 燃料电池 (H3.1.1 PEMFC, H3.1.2 SOFC, H3.1.3 AFC)
- H3.2 氢能交通 (H3.2.1汽车, H3.2.2轨道/船舶/航空, H3.2.3车载供氢)
- H3.3 工业用氢 (H3.3.1氢冶金, H3.3.2石化/合成氨, H3.3.3高纯氢)
- H3.4 氢能发电与储能 (H3.4.1燃气轮机, H3.4.2双向储能, H3.4.3电网调峰)
- H3.5 氢基燃料合成 (H3.5.1绿氨, H3.5.2电子甲醇, H3.5.3航空煤油)

## 输出格式

```json
[
  {{
    "patent_id": "专利ID",
    "tech_domains": ["H1.1.2", "H2.1.2"],
    "confidence": 0.85,
    "reasoning": "该专利涉及质子交换膜电解制氢和高压储氢技术"
  }}
]
```

只返回JSON。"""

        response = self.call_llm(prompt)
        if not response:
            return []

        try:
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                results = json.loads(json_str)
                return results if isinstance(results, list) else []
        except:
            return []

    def analyze_tech_similarity(self, patents: List[Dict]) -> List[Dict]:
        """分析技术相似性"""
        if len(patents) < 2:
            return []

        patents = patents[:6]  # 限制数量

        patents_info = []
        for i, p in enumerate(patents):
            patents_info.append(
                f"**[{i + 1}]** ID: {p['patentId']}\n"
                f"标题: {p['titleZh']}\n"
                f"摘要: {p['abstractZh'][:200]}"
            )

        prompt = f"""分析以下专利间的技术相似性。

{chr(10).join(patents_info)}

## 分析维度

1. **技术路线**: 是否使用相同/相似的技术方案
2. **应用场景**: 目标应用领域是否重叠
3. **核心创新**: 解决的技术问题是否相关
4. **产业链位置**: 处于产业链的相同环节

## 输出格式

```json
[
  {{
    "patent_id_1": "专利ID1",
    "patent_id_2": "专利ID2",
    "similarity_score": 0.75,
    "relationship_type": "技术路线相似",
    "dimension": "制氢技术",
    "reason": "都采用质子交换膜电解技术"
  }}
]
```

只输出相似度>0.6的专利对。只返回JSON。"""

        response = self.call_llm(prompt)
        if not response:
            return []

        try:
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                results = json.loads(json_str)
                return results if isinstance(results, list) else []
        except:
            return []

    def extract_location(self, patents: List[Dict]) -> List[Dict]:
        """提取地理位置信息"""
        patents_info = []
        for i, p in enumerate(patents[:5]):
            patents_info.append(
                f"### 专利{i + 1}\n"
                f"ID: {p['patentId']}\n"
                f"标题: {p['titleZh']}\n"
                f"摘要前段: {p['abstractZh'][:200] if p.get('abstractZh') else ''}"
            )

        prompt = f"""从以下专利中提取地理位置信息。

{chr(10).join(patents_info)}

## 提取规则

1. 识别专利申请人/权利人地址中的省市县信息
2. 识别技术研发或应用地点
3. 优先级: 申请人地址 > 明确地名

## 输出格式

```json
[
  {{
    "patent_id": "专利ID",
    "locations": [
      {{
        "province": "江苏省",
        "city": "南京市",
        "district": null,
        "confidence": 0.9,
        "source": "专利内容"
      }}
    ]
  }}
]
```

如果无法识别地点,locations为空数组。只返回JSON。"""

        response = self.call_llm(prompt)
        if not response:
            return []

        try:
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                results = json.loads(json_str)
                return results if isinstance(results, list) else []
        except:
            return []

    def process_patent_batch(self, patents: List[Dict]) -> Dict:
        """处理一批专利 - 每个小批次完成后立即保存"""
        if self.should_stop:
            return None

        try:
            results = {
                'patent_ids': [p['patentId'] for p in patents],
                'green_classifications': [],
                'tech_classifications': [],
                'tech_similarities': [],
                'location_data': []
            }

            # 绿色技术分类
            if LLM_ENHANCE_CONFIG.get('enable_green_classification', True):
                green_results = self.classify_green_technology(patents)
                if green_results:
                    results['green_classifications'] = green_results
                    # 立即保存
                    self.storage.append_data('green_classifications', green_results)

            # 技术领域细化
            if LLM_ENHANCE_CONFIG.get('enable_tech_classification', True):
                tech_results = self.classify_tech_domain(patents)
                if tech_results:
                    results['tech_classifications'] = tech_results
                    # 立即保存
                    self.storage.append_data('tech_classifications', tech_results)

            # 技术相似性分析
            if LLM_ENHANCE_CONFIG.get('enable_semantic_relations', True):
                similarity_results = self.analyze_tech_similarity(patents)
                if similarity_results:
                    results['tech_similarities'] = similarity_results
                    # 立即保存
                    self.storage.append_data('tech_similarities', similarity_results)

            # 地理位置提取
            if LLM_ENHANCE_CONFIG.get('enable_location_extraction', True):
                location_results = self.extract_location(patents)
                if location_results:
                    results['location_data'] = location_results
                    # 立即保存
                    self.storage.append_data('location_data', location_results)

            return results

        except Exception as e:
            logger.error(f"批次处理失败: {e}")
            return None

    def generate_all_data(self, max_patents: Optional[int] = None):
        """生成所有数据并保存到JSON - 支持实时保存"""
        logger.info("=" * 60)
        logger.info("开始LLM数据生成")
        logger.info("=" * 60)

        total_patents = self.get_total_patents_count()
        already_processed = self.storage.get_processed_count()
        remaining = total_patents - already_processed

        logger.info(f"专利总数: {total_patents}")
        logger.info(f"已处理: {already_processed}")
        logger.info(f"剩余: {remaining}")

        if max_patents:
            remaining = min(remaining, max_patents)
            logger.info(f"本次处理: {remaining} 个专利")

        if remaining == 0:
            logger.info("所有专利已处理完成!")
            self._print_stats()
            return

        batch_size = LLM_ENHANCE_CONFIG['batch_size']
        max_workers = LLM_ENHANCE_CONFIG.get('max_workers', 3)

        processed_count = 0

        while processed_count < remaining and not self.should_stop:
            # 获取未处理的专利
            patents_batch = self.get_unprocessed_patents(limit=batch_size * max_workers)
            if not patents_batch:
                break

            logger.info(f"\n处理批次: {len(patents_batch)} 个专利")

            # 并发处理
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = []

                # 分小批次提交
                for i in range(0, len(patents_batch), batch_size):
                    if self.should_stop:
                        break
                    sub_batch = patents_batch[i:i + batch_size]
                    future = executor.submit(self.process_patent_batch, sub_batch)
                    futures.append((future, sub_batch))

                # 收集结果
                for future, sub_batch in futures:
                    if self.should_stop:
                        break

                    try:
                        results = future.result(timeout=300)

                        if results:
                            # 数据已在 process_patent_batch 中实时保存
                            # 这里只需标记专利为已处理
                            self.storage.mark_processed(results['patent_ids'])
                            processed_count += len(results['patent_ids'])

                            logger.info(f"✓ 进度: {already_processed + processed_count}/{total_patents} " +
                                        f"(本次: {processed_count}/{remaining})")

                    except Exception as e:
                        logger.error(f"批次处理失败: {e}")

            # 控制速率
            if not self.should_stop:
                time.sleep(1)

        logger.info("=" * 60)
        if self.should_stop:
            logger.info("⚠️ 处理已中断，进度已保存")
        else:
            logger.info("✓ LLM数据生成完成!")
        self._print_stats()
        logger.info("=" * 60)

    def _print_stats(self):
        """打印统计信息"""
        stats = self.storage.get_stats()
        logger.info(f"会话ID: {stats['session_id']}")
        logger.info(f"  - 已处理专利: {stats['processed_patents']} 个")
        logger.info(f"  - 绿色分类: {stats['green_classifications']} 条")
        logger.info(f"  - 技术领域: {stats['tech_classifications']} 条")
        logger.info(f"  - 相似关系: {stats['tech_similarities']} 条")
        logger.info(f"  - 地理位置: {stats['location_data']} 条")
        logger.info(f"\nJSON文件保存在: {self.storage.output_dir}")


def find_latest_session(output_dir: str) -> Optional[str]:
    """查找最新的会话"""
    if not os.path.exists(output_dir):
        return None

    sessions = []
    for filename in os.listdir(output_dir):
        if filename.endswith('_progress.json'):
            session_id = filename.replace('_progress.json', '')
            filepath = os.path.join(output_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    progress = json.load(f)
                    sessions.append({
                        'session_id': session_id,
                        'processed_count': len(progress.get('processed_patents', [])),
                        'last_update': progress.get('last_update', ''),
                        'filepath': filepath,
                        'mtime': os.path.getmtime(filepath)
                    })
            except:
                pass

    if not sessions:
        return None

    # 返回最新修改的会话
    latest = sorted(sessions, key=lambda x: x['mtime'], reverse=True)[0]
    logger.info(f"找到最新会话: {latest['session_id']} (已处理 {latest['processed_count']} 个专利)")
    return latest['session_id']


def main():
    """主函数"""
    session_id = None
    max_patents = None
    force_new = False

    # 解析命令行参数
    i = 1
    while i < len(sys.argv):
        if sys.argv[i] == '--session':
            session_id = sys.argv[i + 1] if i + 1 < len(sys.argv) else None
            i += 2
        elif sys.argv[i] == '--max':
            max_patents = int(sys.argv[i + 1]) if i + 1 < len(sys.argv) else None
            i += 2
        elif sys.argv[i] == '--new':
            force_new = True
            i += 1
        elif sys.argv[i] == '--help':
            print("""
用法: python llm_generate_json_improved.py [选项]

选项:
  --session SESSION_ID   指定会话ID继续处理
  --max N               最多处理N个专利
  --new                 强制创建新会话
  --help                显示此帮助信息

默认行为:
  - 如果存在未完成的会话,自动继续最新会话
  - 如果不存在会话,创建新会话
  - 随时可以用 Ctrl+C 中断,进度会自动保存
  - 再次运行即可从断点继续

示例:
  python llm_generate_json_improved.py              # 自动恢复或新建
  python llm_generate_json_improved.py --new        # 强制创建新会话
  python llm_generate_json_improved.py --max 100    # 最多处理100个专利
  python llm_generate_json_improved.py --session 20251123_114621  # 继续指定会话
            """)
            return
        else:
            i += 1

    # 如果没有指定会话且不是强制新建，自动查找最新会话
    if not session_id and not force_new:
        latest_session = find_latest_session(LLM_ENHANCE_CONFIG['output_dir'])
        if latest_session:
            session_id = latest_session
            logger.info(f"🔄 自动恢复最新会话: {session_id}")
        else:
            logger.info("📝 未找到现有会话，创建新会话")

    generator = LLMDataGenerator(session_id=session_id)

    try:
        generator.generate_all_data(max_patents=max_patents)

        logger.info("\n✓ 数据生成完成!")
        logger.info(f"下一步: 运行 llm_import_to_neo4j.py --session {generator.session_id}")

    except KeyboardInterrupt:
        logger.info("\n⚠️ 用户中断，进度已自动保存")
        logger.info(f"💡 直接再次运行即可继续: python llm_generate_json_improved.py")
        logger.info(f"   或指定会话: python llm_generate_json_improved.py --session {generator.session_id}")
    except Exception as e:
        logger.error(f"错误: {e}", exc_info=True)
        raise
    finally:
        generator.close()


if __name__ == "__main__":
    main()




llm_import_to_neo4j.py:
# -*- coding: utf-8 -*-
"""
LLM数据导入模块 - 从JSON批量导入到Neo4j
功能:
1. 读取JSON文件中的所有数据
2. 批量导入到Neo4j数据库
3. 创建节点和关系
4. 提供详细的导入报告
"""

import os
import json
import logging
from datetime import datetime
from neo4j import GraphDatabase
from config import NEO4J_CONFIG, LLM_ENHANCE_CONFIG
from typing import List, Dict, Optional

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('llm_import_to_neo4j.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class Neo4jImporter:
    """Neo4j数据导入器"""

    def __init__(self, session_id: str, output_dir: str = None):
        """初始化"""
        self.session_id = session_id
        self.output_dir = output_dir or LLM_ENHANCE_CONFIG['output_dir']

        # Neo4j连接
        self.driver = GraphDatabase.driver(
            NEO4J_CONFIG['uri'],
            auth=(NEO4J_CONFIG['user'], NEO4J_CONFIG['password']),
            max_connection_lifetime=3600,
            max_connection_pool_size=50,
            keep_alive=True
        )

        # JSON文件路径
        self.files = {
            'green_classifications': os.path.join(self.output_dir, f'{session_id}_green_classifications.json'),
            'tech_classifications': os.path.join(self.output_dir, f'{session_id}_tech_classifications.json'),
            'tech_similarities': os.path.join(self.output_dir, f'{session_id}_tech_similarities.json'),
            'location_data': os.path.join(self.output_dir, f'{session_id}_location_data.json'),
            'progress': os.path.join(self.output_dir, f'{session_id}_progress.json')
        }

        # 导入统计
        self.import_stats = {
            'green_classifications': 0,
            'tech_classifications': 0,
            'tech_similarities': 0,
            'location_data': 0,
            'errors': []
        }

    def close(self):
        """关闭连接"""
        self.driver.close()

    def load_json_data(self, data_type: str) -> List[Dict]:
        """加载JSON数据"""
        filepath = self.files[data_type]

        if not os.path.exists(filepath):
            logger.warning(f"文件不存在: {filepath}")
            return []

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"✓ 加载 {data_type}: {len(data) if isinstance(data, list) else 'N/A'} 条记录")
                return data if isinstance(data, list) else []
        except Exception as e:
            logger.error(f"加载 {data_type} 失败: {e}")
            self.import_stats['errors'].append(f"加载{data_type}失败: {e}")
            return []

    def import_green_classifications(self):
        """导入绿色技术分类"""
        logger.info("\n" + "=" * 60)
        logger.info("导入绿色技术分类")
        logger.info("=" * 60)

        data = self.load_json_data('green_classifications')
        if not data:
            logger.info("没有绿色技术分类数据需要导入")
            return

        with self.driver.session() as session:
            # 批量导入
            batch_size = 1000
            for i in range(0, len(data), batch_size):
                batch = data[i:i + batch_size]

                try:
                    query = """
                    UNWIND $batch AS item
                    MATCH (p:Patent {patentId: item.patent_id})
                    MERGE (g:GreenCategory {code: item.category_code})
                    ON CREATE SET g.name = item.category_name
                    MERGE (p)-[r:CLASSIFIED_AS]->(g)
                    SET r.confidence = item.confidence,
                        r.reasoning = item.reasoning,
                        r.discovered_by = 'LLM',
                        r.discovered_at = datetime()
                    """
                    result = session.run(query, batch=batch)
                    result.consume()

                    self.import_stats['green_classifications'] += len(batch)
                    logger.info(f"已导入: {self.import_stats['green_classifications']}/{len(data)}")

                except Exception as e:
                    logger.error(f"导入绿色分类批次失败: {e}")
                    self.import_stats['errors'].append(f"绿色分类批次{i}-{i + batch_size}失败: {e}")

        logger.info(f"✓ 绿色技术分类导入完成: {self.import_stats['green_classifications']} 条")

    def import_tech_classifications(self):
        """导入技术领域分类"""
        logger.info("\n" + "=" * 60)
        logger.info("导入技术领域分类")
        logger.info("=" * 60)

        data = self.load_json_data('tech_classifications')
        if not data:
            logger.info("没有技术领域分类数据需要导入")
            return

        # 展开技术领域
        expanded = []
        for item in data:
            for tech_code in item.get('tech_domains', []):
                expanded.append({
                    'patent_id': item['patent_id'],
                    'tech_code': tech_code,
                    'confidence': item.get('confidence', 0.7),
                    'reasoning': item.get('reasoning', '')
                })

        if not expanded:
            logger.info("没有技术领域数据需要导入")
            return

        with self.driver.session() as session:
            batch_size = 1000
            for i in range(0, len(expanded), batch_size):
                batch = expanded[i:i + batch_size]

                try:
                    query = """
                    UNWIND $batch AS item
                    MATCH (p:Patent {patentId: item.patent_id})
                    MATCH (t:TechDomain {code: item.tech_code})
                    MERGE (p)-[r:ALSO_BELONGS_TO]->(t)
                    SET r.confidence = item.confidence,
                        r.reasoning = item.reasoning,
                        r.discovered_by = 'LLM',
                        r.discovered_at = datetime()
                    """
                    result = session.run(query, batch=batch)
                    result.consume()

                    self.import_stats['tech_classifications'] += len(batch)
                    logger.info(f"已导入: {self.import_stats['tech_classifications']}/{len(expanded)}")

                except Exception as e:
                    logger.error(f"导入技术领域批次失败: {e}")
                    self.import_stats['errors'].append(f"技术领域批次{i}-{i + batch_size}失败: {e}")

        logger.info(f"✓ 技术领域分类导入完成: {self.import_stats['tech_classifications']} 条")

    def import_tech_similarities(self):
        """导入技术相似性"""
        logger.info("\n" + "=" * 60)
        logger.info("导入技术相似性关系")
        logger.info("=" * 60)

        data = self.load_json_data('tech_similarities')
        if not data:
            logger.info("没有技术相似性数据需要导入")
            return

        with self.driver.session() as session:
            batch_size = 1000
            for i in range(0, len(data), batch_size):
                batch = data[i:i + batch_size]

                try:
                    query = """
                    UNWIND $batch AS item
                    MATCH (p1:Patent {patentId: item.patent_id_1})
                    MATCH (p2:Patent {patentId: item.patent_id_2})
                    MERGE (p1)-[r:SIMILAR_TO]->(p2)
                    SET r.similarity_score = item.similarity_score,
                        r.relationship_type = item.relationship_type,
                        r.dimension = item.dimension,
                        r.reason = item.reason,
                        r.discovered_by = 'LLM',
                        r.discovered_at = datetime()
                    """
                    result = session.run(query, batch=batch)
                    result.consume()

                    self.import_stats['tech_similarities'] += len(batch)
                    logger.info(f"已导入: {self.import_stats['tech_similarities']}/{len(data)}")

                except Exception as e:
                    logger.error(f"导入相似性批次失败: {e}")
                    self.import_stats['errors'].append(f"相似性批次{i}-{i + batch_size}失败: {e}")

        logger.info(f"✓ 技术相似性导入完成: {self.import_stats['tech_similarities']} 条")

    def import_location_data(self):
        """导入地理位置数据"""
        logger.info("\n" + "=" * 60)
        logger.info("导入地理位置数据")
        logger.info("=" * 60)

        data = self.load_json_data('location_data')
        if not data:
            logger.info("没有地理位置数据需要导入")
            return

        # 展开位置数据
        expanded = []
        for item in data:
            for loc in item.get('locations', []):
                if loc.get('province'):
                    expanded.append({
                        'patent_id': item['patent_id'],
                        'province': loc['province'],
                        'city': loc.get('city'),
                        'district': loc.get('district'),
                        'confidence': loc.get('confidence', 0.7),
                        'source': loc.get('source', 'LLM')
                    })

        if not expanded:
            logger.info("没有地理位置数据需要导入")
            return

        with self.driver.session() as session:
            # 创建省份节点和关系
            logger.info("创建省份节点和关系...")
            batch_size = 1000
            for i in range(0, len(expanded), batch_size):
                batch = expanded[i:i + batch_size]

                try:
                    query = """
                    UNWIND $batch AS item
                    MATCH (p:Patent {patentId: item.patent_id})
                    MERGE (prov:Province {name: item.province})
                    MERGE (p)-[r:LOCATED_IN_PROVINCE_LLM]->(prov)
                    SET r.confidence = item.confidence,
                        r.source = item.source,
                        r.discovered_at = datetime()
                    """
                    result = session.run(query, batch=batch)
                    result.consume()

                    logger.info(f"省份关系: {i + len(batch)}/{len(expanded)}")

                except Exception as e:
                    logger.error(f"导入省份批次失败: {e}")

            # 创建城市节点和关系
            city_data = [item for item in expanded if item.get('city')]
            if city_data:
                logger.info("创建城市节点和关系...")
                for i in range(0, len(city_data), batch_size):
                    batch = city_data[i:i + batch_size]

                    try:
                        query = """
                        UNWIND $batch AS item
                        MATCH (p:Patent {patentId: item.patent_id})
                        MERGE (c:City {fullName: item.province + '-' + item.city})
                        ON CREATE SET c.name = item.city, c.province = item.province
                        MERGE (p)-[r:LOCATED_IN_CITY_LLM]->(c)
                        SET r.confidence = item.confidence,
                            r.source = item.source,
                            r.discovered_at = datetime()
                        """
                        result = session.run(query, batch=batch)
                        result.consume()

                        logger.info(f"城市关系: {i + len(batch)}/{len(city_data)}")

                    except Exception as e:
                        logger.error(f"导入城市批次失败: {e}")

            # 创建区县节点和关系
            district_data = [item for item in expanded if item.get('district')]
            if district_data:
                logger.info("创建区县节点和关系...")
                for i in range(0, len(district_data), batch_size):
                    batch = district_data[i:i + batch_size]

                    try:
                        query = """
                        UNWIND $batch AS item
                        MATCH (p:Patent {patentId: item.patent_id})
                        MERGE (d:District {fullName: item.province + '-' + item.city + '-' + item.district})
                        ON CREATE SET d.name = item.district,
                                     d.city = item.city,
                                     d.province = item.province
                        MERGE (p)-[r:LOCATED_IN_DISTRICT_LLM]->(d)
                        SET r.confidence = item.confidence,
                            r.source = item.source,
                            r.discovered_at = datetime()
                        """
                        result = session.run(query, batch=batch)
                        result.consume()

                        logger.info(f"区县关系: {i + len(batch)}/{len(district_data)}")

                    except Exception as e:
                        logger.error(f"导入区县批次失败: {e}")

            self.import_stats['location_data'] = len(expanded)

        logger.info(f"✓ 地理位置数据导入完成: {len(expanded)} 条")

    def verify_import(self):
        """验证导入结果"""
        logger.info("\n" + "=" * 60)
        logger.info("验证导入结果")
        logger.info("=" * 60)

        with self.driver.session() as session:
            queries = {
                "绿色技术分类关系": "MATCH ()-[r:CLASSIFIED_AS]->() WHERE r.discovered_by = 'LLM' RETURN count(r) as count",
                "技术领域关系": "MATCH ()-[r:ALSO_BELONGS_TO]->() WHERE r.discovered_by = 'LLM' RETURN count(r) as count",
                "相似性关系": "MATCH ()-[r:SIMILAR_TO]->() WHERE r.discovered_by = 'LLM' RETURN count(r) as count",
                "省份关系(LLM)": "MATCH ()-[r:LOCATED_IN_PROVINCE_LLM]->() RETURN count(r) as count",
                "城市关系(LLM)": "MATCH ()-[r:LOCATED_IN_CITY_LLM]->() RETURN count(r) as count",
                "区县关系(LLM)": "MATCH ()-[r:LOCATED_IN_DISTRICT_LLM]->() RETURN count(r) as count"
            }

            for name, query in queries.items():
                try:
                    result = session.run(query)
                    count = result.single()['count']
                    logger.info(f"  {name}: {count} 条")
                except Exception as e:
                    logger.error(f"验证 {name} 失败: {e}")

    def import_all(self):
        """导入所有数据"""
        logger.info("=" * 60)
        logger.info("开始从JSON导入到Neo4j")
        logger.info(f"会话ID: {self.session_id}")
        logger.info("=" * 60)

        start_time = datetime.now()

        # 依次导入各类数据
        self.import_green_classifications()
        self.import_tech_classifications()
        self.import_tech_similarities()
        self.import_location_data()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        # 验证导入
        self.verify_import()

        # 打印总结
        logger.info("\n" + "=" * 60)
        logger.info("导入完成总结")
        logger.info("=" * 60)
        logger.info(f"会话ID: {self.session_id}")
        logger.info(f"耗时: {duration:.2f} 秒")
        logger.info(f"\n导入统计:")
        logger.info(f"  - 绿色技术分类: {self.import_stats['green_classifications']} 条")
        logger.info(f"  - 技术领域分类: {self.import_stats['tech_classifications']} 条")
        logger.info(f"  - 技术相似性: {self.import_stats['tech_similarities']} 条")
        logger.info(f"  - 地理位置: {self.import_stats['location_data']} 条")

        if self.import_stats['errors']:
            logger.warning(f"\n错误数量: {len(self.import_stats['errors'])}")
            for error in self.import_stats['errors'][:10]:  # 只显示前10个错误
                logger.warning(f"  - {error}")
        else:
            logger.info("\n✓ 没有错误!")

        logger.info("=" * 60)


def list_available_sessions(output_dir: str = None):
    """列出所有可用的会话"""
    output_dir = output_dir or LLM_ENHANCE_CONFIG['output_dir']

    if not os.path.exists(output_dir):
        logger.info(f"输出目录不存在: {output_dir}")
        return []

    # 查找所有progress.json文件
    sessions = []
    for filename in os.listdir(output_dir):
        if filename.endswith('_progress.json'):
            session_id = filename.replace('_progress.json', '')

            # 读取进度信息
            filepath = os.path.join(output_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    progress = json.load(f)
                    sessions.append({
                        'session_id': session_id,
                        'processed_patents': len(progress.get('processed_patents', [])),
                        'last_update': progress.get('last_update', 'Unknown')
                    })
            except:
                pass

    return sorted(sessions, key=lambda x: x['session_id'], reverse=True)


def main():
    """主函数"""
    import sys

    session_id = None
    list_sessions = False

    if len(sys.argv) > 1:
        if sys.argv[1] == '--session':
            session_id = sys.argv[2] if len(sys.argv) > 2 else None
        elif sys.argv[1] == '--list':
            list_sessions = True

    if list_sessions:
        logger.info("可用的会话:")
        sessions = list_available_sessions()
        if not sessions:
            logger.info("  (没有找到会话)")
        else:
            for sess in sessions:
                logger.info(f"  - {sess['session_id']}")
                logger.info(f"    已处理专利: {sess['processed_patents']}")
                logger.info(f"    最后更新: {sess['last_update']}")
        return

    if not session_id:
        # 尝试使用最新的会话
        sessions = list_available_sessions()
        if sessions:
            session_id = sessions[0]['session_id']
            logger.info(f"使用最新会话: {session_id}")
        else:
            logger.error("错误: 请指定会话ID")
            logger.info("用法: python llm_import_to_neo4j.py --session {session_id}")
            logger.info("或者: python llm_import_to_neo4j.py --list  # 列出所有会话")
            return

    importer = Neo4jImporter(session_id)

    try:
        importer.import_all()
        logger.info("\n✓ 所有数据已成功导入Neo4j!")

    except Exception as e:
        logger.error(f"导入失败: {e}", exc_info=True)
        raise
    finally:
        importer.close()


if __name__ == "__main__":
    main()



tech_domains.py:
# -*- coding: utf-8 -*-
"""
技术领域树定义
三级层次结构: L1(产业链) -> L2(技术路径) -> L3(细分技术)
"""

TECH_TREE = {
    "H1": {
        "nameZh": "制氢(上游)",
        "nameEn": "Hydrogen Production",
        "level": 1,
        "children": {
            "H1.1": {
                "nameZh": "电解水制氢",
                "nameEn": "Water Electrolysis",
                "level": 2,
                "children": {
                    "H1.1.1": {"nameZh": "碱性电解(AEL)", "nameEn": "Alkaline Electrolysis (AEL)", "level": 3},
                    "H1.1.2": {"nameZh": "质子交换膜电解(PEMEL)", "nameEn": "Proton Exchange Membrane Electrolysis (PEMEL)", "level": 3},
                    "H1.1.3": {"nameZh": "固体氧化物电解(SOEC)", "nameEn": "Solid Oxide Electrolysis Cell (SOEC)", "level": 3},
                    "H1.1.4": {"nameZh": "阴离子交换膜电解(AEMEL)", "nameEn": "Anion Exchange Membrane Electrolysis (AEMEL)", "level": 3}
                }
            },
            "H1.2": {
                "nameZh": "化石能源制氢",
                "nameEn": "Fossil-based Reforming",
                "level": 2,
                "children": {
                    "H1.2.1": {"nameZh": "天然气重整(SMR)", "nameEn": "Steam Methane Reforming (SMR)", "level": 3},
                    "H1.2.2": {"nameZh": "煤制氢(煤气化)", "nameEn": "Coal Gasification", "level": 3},
                    "H1.2.3": {"nameZh": "甲醇裂解制氢", "nameEn": "Methanol Reforming", "level": 3}
                }
            },
            "H1.3": {
                "nameZh": "可再生能源耦合制氢",
                "nameEn": "Renewable-integrated Production",
                "level": 2,
                "children": {
                    "H1.3.1": {"nameZh": "风光电制氢系统集成", "nameEn": "Wind-Solar-H2 System Integration", "level": 3},
                    "H1.3.2": {"nameZh": "离网/微网制氢", "nameEn": "Off-grid/Microgrid H2 Production", "level": 3},
                    "H1.3.3": {"nameZh": "电解槽与可再生能源协同控制", "nameEn": "Electrolyzer-Renewable Energy Co-control", "level": 3}
                }
            },
            "H1.4": {
                "nameZh": "光/热/生物制氢",
                "nameEn": "Alternative (Photo/Thermo/Bio) Methods",
                "level": 2,
                "children": {
                    "H1.4.1": {"nameZh": "光催化/光电化学制氢", "nameEn": "Photo-catalytic/Photoelectrochemical H2", "level": 3},
                    "H1.4.2": {"nameZh": "太阳能热化学循环制氢", "nameEn": "Solar Thermochemical Cycles", "level": 3},
                    "H1.4.3": {"nameZh": "生物质气化/发酵制氢", "nameEn": "Biomass Gasification/Fermentation", "level": 3}
                }
            }
        }
    },
    "H2": {
        "nameZh": "储运氢(中游)",
        "nameEn": "Hydrogen Storage & Transportation",
        "level": 1,
        "children": {
            "H2.1": {
                "nameZh": "高压气态储运",
                "nameEn": "High-pressure Gaseous",
                "level": 2,
                "children": {
                    "H2.1.1": {"nameZh": "35 MPa 储氢容器", "nameEn": "35 MPa H2 Containers", "level": 3},
                    "H2.1.2": {"nameZh": "70 MPa 储氢容器", "nameEn": "70 MPa H2 Containers", "level": 3},
                    "H2.1.3": {"nameZh": "高压管束车/长管拖车", "nameEn": "High-pressure Tube Trailers", "level": 3}
                }
            },
            "H2.2": {
                "nameZh": "低温液态储运",
                "nameEn": "Cryogenic Liquid",
                "level": 2,
                "children": {
                    "H2.2.1": {"nameZh": "液氢储罐(车载/固定)", "nameEn": "Liquid H2 Tanks (Mobile/Stationary)", "level": 3},
                    "H2.2.2": {"nameZh": "液氢加注与转注技术", "nameEn": "Liquid H2 Transfer Technology", "level": 3},
                    "H2.2.3": {"nameZh": "液氢蒸发损失控制", "nameEn": "Liquid H2 Boil-off Control", "level": 3}
                }
            },
            "H2.3": {
                "nameZh": "固态/材料储氢",
                "nameEn": "Solid-state / Material-based",
                "level": 2,
                "children": {
                    "H2.3.1": {"nameZh": "金属/合金氢化物", "nameEn": "Metal/Alloy Hydrides", "level": 3},
                    "H2.3.2": {"nameZh": "化学氢化物(NaBH₄, NH₃BH₃ 等)", "nameEn": "Chemical Hydrides", "level": 3},
                    "H2.3.3": {"nameZh": "多孔材料吸附储氢(MOFs, 碳材料)", "nameEn": "Porous Materials (MOFs, Carbon)", "level": 3}
                }
            },
            "H2.4": {
                "nameZh": "有机载体储运(LOHC等)",
                "nameEn": "Liquid Organic Carriers",
                "level": 2,
                "children": {
                    "H2.4.1": {"nameZh": "甲苯/甲基环己烷体系", "nameEn": "Toluene/Methylcyclohexane System", "level": 3},
                    "H2.4.2": {"nameZh": "N-乙基咔唑等杂环体系", "nameEn": "N-Ethylcarbazole Systems", "level": 3},
                    "H2.4.3": {"nameZh": "载体加氢/脱氢催化剂", "nameEn": "Hydrogenation/Dehydrogenation Catalysts", "level": 3}
                }
            },
            "H2.5": {
                "nameZh": "氢输配基础设施",
                "nameEn": "Distribution Infrastructure",
                "level": 2,
                "children": {
                    "H2.5.1": {"nameZh": "氢气管道(纯氢/掺氢)", "nameEn": "H2 Pipelines (Pure/Blended)", "level": 3},
                    "H2.5.2": {"nameZh": "加氢站关键设备", "nameEn": "H2 Refueling Station Equipment", "level": 3},
                    "H2.5.3": {"nameZh": "氢气压缩与纯化", "nameEn": "H2 Compression & Purification", "level": 3}
                }
            }
        }
    },
    "H3": {
        "nameZh": "用氢(下游)",
        "nameEn": "Hydrogen Utilization",
        "level": 1,
        "children": {
            "H3.1": {
                "nameZh": "燃料电池",
                "nameEn": "Fuel Cells",
                "level": 2,
                "children": {
                    "H3.1.1": {"nameZh": "质子交换膜燃料电池(PEMFC)", "nameEn": "Proton Exchange Membrane Fuel Cell (PEMFC)", "level": 3},
                    "H3.1.2": {"nameZh": "固体氧化物燃料电池(SOFC)", "nameEn": "Solid Oxide Fuel Cell (SOFC)", "level": 3},
                    "H3.1.3": {"nameZh": "碱性燃料电池(AFC)等其他类型", "nameEn": "Alkaline Fuel Cell (AFC) & Others", "level": 3}
                }
            },
            "H3.2": {
                "nameZh": "氢能交通",
                "nameEn": "Hydrogen Mobility",
                "level": 2,
                "children": {
                    "H3.2.1": {"nameZh": "氢燃料电池汽车", "nameEn": "Fuel Cell Electric Vehicles", "level": 3},
                    "H3.2.2": {"nameZh": "氢动力轨道交通/船舶/航空", "nameEn": "H2 Rail/Marine/Aviation", "level": 3},
                    "H3.2.3": {"nameZh": "车载供氢系统", "nameEn": "On-board H2 Supply Systems", "level": 3}
                }
            },
            "H3.3": {
                "nameZh": "工业用氢",
                "nameEn": "Industrial Applications",
                "level": 2,
                "children": {
                    "H3.3.1": {"nameZh": "氢冶金(直接还原铁等)", "nameEn": "H2 Metallurgy (DRI, etc.)", "level": 3},
                    "H3.3.2": {"nameZh": "石化/合成氨/甲醇用氢", "nameEn": "Petrochemical/Ammonia/Methanol", "level": 3},
                    "H3.3.3": {"nameZh": "电子级高纯氢应用", "nameEn": "Electronics-grade H2", "level": 3}
                }
            },
            "H3.4": {
                "nameZh": "氢能发电与储能",
                "nameEn": "Power Generation & Grid Storage",
                "level": 2,
                "children": {
                    "H3.4.1": {"nameZh": "氢燃气轮机发电", "nameEn": "H2 Gas Turbine Power", "level": 3},
                    "H3.4.2": {"nameZh": "氢-电双向储能系统", "nameEn": "H2-Electricity Bidirectional Storage", "level": 3},
                    "H3.4.3": {"nameZh": "氢参与电网调峰", "nameEn": "H2 Grid Balancing", "level": 3}
                }
            },
            "H3.5": {
                "nameZh": "氢基燃料合成",
                "nameEn": "Hydrogen-derived Fuels",
                "level": 2,
                "children": {
                    "H3.5.1": {"nameZh": "绿氨合成", "nameEn": "Green Ammonia Synthesis", "level": 3},
                    "H3.5.2": {"nameZh": "电子甲醇/e-Fuels", "nameEn": "e-Methanol/e-Fuels", "level": 3},
                    "H3.5.3": {"nameZh": "合成航空煤油(e-Kerosene)", "nameEn": "e-Kerosene", "level": 3}
                }
            }
        }
    }
}


def get_all_tech_domains():
    """
    展开技术领域树，返回所有节点的列表
    每个节点包含: code, nameZh, nameEn, level, parent_code
    """
    domains = []

    def traverse(node_dict, parent_code=None):
        for code, data in node_dict.items():
            domain = {
                "code": code,
                "nameZh": data["nameZh"],
                "nameEn": data["nameEn"],
                "level": data["level"],
                "parent_code": parent_code
            }
            domains.append(domain)

            if "children" in data:
                traverse(data["children"], parent_code=code)

    traverse(TECH_TREE)
    return domains


if __name__ == "__main__":
    # 测试
    domains = get_all_tech_domains()
    print(f"共有 {len(domains)} 个技术领域节点")
    for d in domains[:10]:
        print(d)




